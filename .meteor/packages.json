{
  "name": "clmtrackr",
  "version": "0.0.1",
  "description": "Javascript library for precise tracking of facial features via Constrained Local Models",
  "scripts": {
    "build": "browserify js/index.js -s clm -o ./clmtrackr.js; uglifyjs ./clmtrackr.js -m -r 'clm,clmtrackr' -o ./clmtrackr.min.js"
  },
  "main": "js/index.js",
  "repository": {
    "type": "git",
    "url": "https://github.com/auduno/clmtrackr.git"
  },
  "devDependencies": {
    "browserify": "^13.0.0",
    "getusermedia": "^1.3.5",
    "exdat": "^0.6.2",
    "blob.js": "git+https://github.com/andyinabox/Blob.js",
    "filesaver.js": "git+https://github.com/andyinabox/FileSaver.js",
    "uglify": "^0.1.5",
    "raf": "^3.1.0",
    "stats.js": "0.0.14-master"
  },
  "dependencies": {
    "jsfeat": "0.0.8",
    "numeric": "^1.2.6"
  },
  "gitHead": "8b99e14d35fe195e91680ed70598c4941dda00b1",
  "readme": "clmtrackr\n======\n\n![tracked face](http://auduno.github.com/clmtrackr/media/clmtrackr_03.jpg)\n\nnpm fork\n--------\n\nThis fork exposes the clmtrackr library for use with npm. It can be used as a browser global as seen in examples, or via browserify etc as an npm module. To install:\n\n```bash\nnpm install andyinabox/clmtrackr\n```\nA pull request has been submitted to the main project, but I'm not sure if/when it will be integrated so this will do for now.\n\n---\n\n**clmtrackr** is a javascript library for fitting facial models to faces in videos or images. It currently is an implementation of *constrained local models* fitted by *regularized landmark mean-shift*, as described in [Jason M. Saragih's paper](http://dl.acm.org/citation.cfm?id=1938021). **clmtrackr** tracks a face and outputs the coordinate positions of the face model as an array, following the numbering of the model below:\n\n[![facemodel_numbering](http://auduno.github.com/clmtrackr/media/facemodel_numbering_new_small.png)](http://auduno.github.com/clmtrackr/media/facemodel_numbering_new.png)\n\n[Reference](http://auduno.github.io/clmtrackr/docs/reference.html) - [Overview](http://auduno.tumblr.com/post/61888277175/fitting-faces)\n\nThe library provides some generic face models that were trained on [the MUCT database](http://www.milbo.org/muct/) and some additional self-annotated images. Check out [clmtools](https://github.com/auduno/clmtools) for building your own models.\n\nThe library requires [jsfeat.js](https://github.com/inspirit/jsfeat) (for initial face detection) and [numeric.js](http://numericjs.com) (for matrix math).\n\nFor tracking in video, it is recommended to use a browser with WebGL support, though the library should work on any modern browser.\n\nFor some more information about Constrained Local Models, take a look at Xiaoguang Yan's [excellent tutorial](https://sites.google.com/site/xgyanhome/home/projects/clm-implementation/ConstrainedLocalModel-tutorial%2Cv0.7.pdf?attredirects=0), which was of great help in implementing this library.\n\n### Examples ###\n\n* [Tracking in image](https://auduno.github.io/clmtrackr/clm_image.html)\n* [Tracking in video](https://auduno.github.io/clmtrackr/clm_video.html)\n* [Face substitution](https://auduno.github.io/clmtrackr/examples/facesubstitution.html)\n* [Face masking](https://auduno.github.io/clmtrackr/face_mask.html)\n* [Realtime face deformation](https://auduno.github.io/clmtrackr/examples/facedeform.html)\n* [Emotion detection](https://auduno.github.io/clmtrackr/examples/clm_emotiondetection.html)\n* [Caricature](https://auduno.github.io/clmtrackr/examples/caricature.html)\n\n### Usage ###\n\nDownload the minified library [clmtrackr.js](https://github.com/auduno/clmtrackr/raw/dev/clmtrackr.js) and one of the models, and include them in your webpage. **clmtrackr** depends on [*numeric.js*](https://github.com/sloisel/numeric/) and [*jsfeat.js*](https://github.com/inspirit/jsfeat), but these are included in the minified library.\n\n```html\n/* clmtrackr libraries */\n<script src=\"js/clmtrackr.js\"></script>\n<script src=\"js/model_pca_20_svm.js\"></script>\n```\n\nThe following code initiates the clmtrackr with the model we included, and starts the tracker running on a video element.\n\n```html\n<video id=\"inputVideo\" width=\"400\" height=\"300\" autoplay loop>\n  <source src=\"./media/somevideo.ogv\" type=\"video/ogg\"/>\n</video>\n<script type=\"text/javascript\">\n  var videoInput = document.getElementById('inputVideo');\n  \n  var ctracker = new clm.tracker();\n  ctracker.init(pModel);\n  ctracker.start(videoInput);\n</script>\n```\n\nYou can now get the positions of the tracked facial features as an array via ```getCurrentPosition()```:\n\n```html\n<script type=\"text/javascript\">\n  function positionLoop() {\n    requestAnimationFrame(positionLoop);\n    var positions = ctracker.getCurrentPosition();\n    // positions = [[x_0, y_0], [x_1,y_1], ... ]\n    // do something with the positions ...\n  }\n  positionLoop();\n</script>\n```\n\nYou can also use the built in function ```draw()``` to draw the tracked facial model on a canvas :\n\n```html\n<canvas id=\"drawCanvas\" width=\"400\" height=\"300\"></canvas>\n<script type=\"text/javascript\">\n  var canvasInput = document.getElementById('canvas');\n  var cc = canvasInput.getContext('2d');\n  function drawLoop() {\n    requestAnimationFrame(drawLoop);\n    cc.clearRect(0, 0, canvasInput.width, canvasInput.height);\n    ctracker.draw(canvasInput);\n  }\n  drawLoop();\n</script>\n```\n\nSee the complete example [here](https://auduno.github.com/clmtrackr/example.html).\n\n### License ###\n\n**clmtrackr** is distributed under the [MIT License](http://www.opensource.org/licenses/MIT)\n",
  "readmeFilename": "README.md",
  "bugs": {
    "url": "https://github.com/auduno/clmtrackr/issues"
  },
  "homepage": "https://github.com/auduno/clmtrackr",
  "_id": "clmtrackr@0.0.1",
  "_shasum": "3890edbf8db74c3fb418c9a310b3b0223b67aa4f",
  "_from": "andyinabox/clmtrackr",
  "_resolved": "git://github.com/andyinabox/clmtrackr.git#8b99e14d35fe195e91680ed70598c4941dda00b1"
}